import cv2
import numpy as np
import torch
import os
from collections import deque
from ultralytics import YOLO
from google.colab import files

class ObjectTracker:
    def __init__(self, max_objects=50, min_confidence=0.5):
        self.max_objects = max_objects
        self.min_confidence = min_confidence
        self.tracks = []
        self.object_count = 0

    def update(self, detections):
        current_detections = []
        for det in detections:
            x1, y1, x2, y2, conf, cls = det
            if conf > self.min_confidence:
                w, h = x2 - x1, y2 - y1
                center = (int(x1 + w / 2), int(y1 + h / 2))
                bbox = (int(x1), int(y1), int(w), int(h))
                current_detections.append((center, bbox, int(cls)))

        new_tracks = []
        for detection in current_detections:
            center, bbox, cls = detection
            matched = False
            for track in self.tracks:
                if track['class'] == cls and len(track['positions']) > 0:
                    last_pos = track['positions'][-1]
                    distance = np.sqrt((center[0] - last_pos[0])**2 + (center[1] - last_pos[1])**2)
                    if distance < 50:
                        track['positions'].append(center)
                        track['bbox'] = bbox
                        if len(track['positions']) > 20:
                            track['positions'].popleft()
                        new_tracks.append(track)
                        matched = True
                        break

            if not matched:
                self.object_count += 1
                new_tracks.append({
                    'id': self.object_count,
                    'positions': deque([center], maxlen=20),
                    'bbox': bbox,
                    'class': cls
                })

        self.tracks = new_tracks
        return len(self.tracks)

def process_video(video_path, output_path=None):
    model = YOLO('yolov8n.pt')
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Error: Could not open video file")
        return

    frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    out = None
    if output_path:
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

    tracker = ObjectTracker()
    frame_count = 0
    class_names = model.names

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        results = model(frame)
        detections = results[0].boxes.data.cpu().numpy()
        object_count = tracker.update(detections)

        for track in tracker.tracks:
            x, y, w, h = track['bbox']
            class_name = class_names[track['class']]
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            cv2.putText(frame, f"ID: {track['id']} ({class_name})", (x, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
            for i in range(1, len(track['positions'])):
                if track['positions'][i - 1] and track['positions'][i]:
                    cv2.line(frame, track['positions'][i - 1], track['positions'][i], (0, 0, 255), 2)

        cv2.putText(frame, f"Objects: {object_count}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.putText(frame, f"Frame: {frame_count}", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

        if out:
            out.write(frame)

    cap.release()
    if out:
        out.release()

    print(f"Total frames processed: {frame_count}")
    if output_path:
        files.download(output_path)

# Upload and process video
uploaded = files.upload()
video_path = list(uploaded.keys())[0]
output_path = "output_video.mp4"
process_video(video_path, output_path)
